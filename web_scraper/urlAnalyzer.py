from termcolor import colored
import argparse
import re
import os
import requests
from bs4 import BeautifulSoup

class URLAnalyzer:
    def __init__(self) -> None:
        # calling get_user_input()
        self.get_user_input()

    # this function will take input from user
    def get_user_input(self):
        parser = argparse.ArgumentParser(description="Usage")
        parser.add_argument('-u', '--url', type=str, help=colored("Single URL you want to analyze.", "green"))
        parser.add_argument('-f', '--file', type=str, help=colored("File containing multiple URLs to analyze.", "green"))
        args = parser.parse_args()

        if args.url and args.file:
            print(colored("[-] Invalid operation: Choose either a URL or a file, not both.", "red"))
            exit()

        if args.url:
            url = args.url
            print(colored(f"[+] Analyzing URL: {url}","yellow"))
            self.is_valid_url(url)

        if args.file:
            file_path = args.file
            self.file_exists(file_path)

    # check if URL is valid
    def is_valid_url(self, url):
        # Regular expression pattern to match URLs
        url_pattern = re.compile(
            r'^(?:http|ftp)s?://'  # http:// or https:// or ftp://
            r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+(?:[A-Z]{2,6}\.?|[A-Z0-9-]{2,}\.?)|'  # domain...
            r'localhost|'  # localhost...
            r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # ...or IP
            r'(?::\d+)?'  # optional port
            r'(?:/?|[/?]\S+)$', re.IGNORECASE)

        if re.match(url_pattern, url):
            print(colored(f"[+] Valid URL: {url}", "green"))
            # if url pass regex check make a get requesr to url
            self.request_url(url)

        else:
            print(colored(f"[-] Invalid URL: {url}", "red"))
    
    def file_exists(self, file_path):
        # Check if the file ends with .txt
        pattern = r'\.txt$'
        match = re.search(pattern, file_path)

        if not match:
            print(colored("[-] Invalid file, check your file path.", "red"))
            print(colored("[-] Only .txt files are allowed.", "red"))
            return

        # Check if the file exists with an absolute path
        if os.path.isfile(file_path):
            print(colored(f"[+] Analyzing file: {file_path}", "yellow"))
            # sorting unique urls
            self.unique_urls(file_path)
            return

        # Check if the file exists in the current directory
        current_dir = os.getcwd()
        complete_file_path = os.path.join(current_dir, file_path)
        if os.path.isfile(complete_file_path):
            print(colored(f"[+] Analyzing file: {complete_file_path}", "yellow"))

            # sorting unique urls
            self.unique_urls(complete_file_path)
            return

        # Error message if the file is not found
        print(colored("[-] File not found, check your file path.", "red"))

    # this function will remove duplicate urls from file
    def unique_urls(self,file_path):
        try:
            # opening file in read mode
            with open(file_path,'r') as file:
                lines = file.readlines()
                unique_lines = list(dict.fromkeys(line.strip() for line in lines))
            
            # Open the file in write mode
            with open(file_path,'w') as file:
                for url in unique_lines:
                    file.write(url + '\n')

            # read urls from file
            self.reading_url_from_file(file_path)

        except FileNotFoundError as f_er:
            print(colored(f"[-] Error: {f_er}","red"))
        except Exception as e:
            print(colored(f"[-] Error: {e}","red"))

    # reading urls from file line by line
    def reading_url_from_file(self,file_path):
        try:
            with open(file_path, 'r') as file:
                for line in file:
                    url = line.strip()
                    # pass url through regex to check wether its a valid url or not
                    self.is_valid_url(url)
        except FileNotFoundError as f_er:
            print(colored(f"[-] Error: {f_er}","red"))
        except Exception as e:
            print(colored(f"[-] Error: {e}","red"))

    # make a get request to url to analyxe html content
    def request_url(self,url):
        try:
            # send a get request
            response = requests.get(url)
            # Check if the request was successful
            if response.status_code == 200:
                html_content = response.text
                soupe = BeautifulSoup(html_content,'html.parser')

                # title = soupe.title.string
                # print(colored(f"[+] Page title: {title}","green"))

                # detect any form
                # self.extract_form(soupe)
                print(soupe)
                self.extract_inputs(soupe)

        except Exception as e:
            print(colored(f"[-] Error: {e}","red"))

        # thi function will extract all usefull data about a web form 
    def extract_form(self,soupe):
        forms = soupe.find_all('form')
        if forms:
            for form in forms:
                action = form.get('action')
                method = form.get('method','get').upper()
                print(colored(f"[+] Form action: {action}, Method: {method}","blue"))
                inputs = form.find_all('input')
                for input_field in inputs:
                    input_type = input_field.get('type')
                    input_name = input_field.get('name')
                    print(colored(f"    [+] Input type: {input_type}, Input name: {input_name}","blue"))
                    if input_type == "hidden":
                        value = input_field.get('value')
                        print(colored(f"    [+] Value: {value}","blue"))
        else:
            print(colored("[-] No HTML form found.","red"))
        
    def extract_inputs(self,soupe):
        inputs = soupe.find_all('input')
        if inputs:
            for input_field in inputs:
                input_type = input_field.get('type')
                input_name = input_field.get('name')
                print(colored(f"[+] Input type: {input_type}, Input name: {input_name}","blue"))
                if input_type == "hidden":
                    value = input_field.get('value')
                    print(colored(f"[+] Value: {value}","blue"))
        else:
            print(colored("[-] No HTML input found.","red"))

obj = URLAnalyzer()


# get all anchor tags
# get all images src and href
# get all scripts and link tags
# get all input fields url
# meta charactors
# hard coded credentials
# javascript
